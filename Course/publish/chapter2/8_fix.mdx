### 1. 自然语言处理流程的顺序是什么？


1. 首先是模型它处理文本并返回原始预测。然后，分词器会对这些预测进行解释，并在将它们转换回文本
2. 首先是 Tokenizer，它处理文本并返回id。模型根据这些 id 并输出预测，可以是一些文本。
3. Tokenizer处理文本并返回 id。模型处理这些 id 并输出一个预测。然后可以再次使用Tokenizer将这些预测转换回某些文本。

### 2. Transformer模型的输出有的张量多少个维度，每个维度分别是什么？


1. 2个维度，分别是: 序列长度(Sequence Length)和批次大小(Batch Size)
2. 2个维度，分别是: 序列长度(Sequence Length)和隐藏层大小(Hidden Size)
3. 3个维度，分别是: 序列长度(Sequence Length)、批次大小(Batch Size)和隐藏层大小(Hidden Size)

### 3.下列哪一个是子词分词的例子（从分词的颗粒度来划分）？


1. WordPiece
2. 基于单个字符的分词
3. 基于空格和标点符号的分割
4. BPE(Byte Pair Encoding)
5. Unigram
6. 以上都不是

### 4.什么是模型头（Haed 层）？


1. 原始Transformer网络的一种组件，直接将张量(Tensors)输入到到正确的层
2. 也称为自注意力(self-attention)机制，它会根据序列的其他标记调整一个标记的表示
3. 一个附加组件，通常由一个或几个层组成，用于将Transformer的预测转换为特定于任务的输出

### 5.什么是AutoModel？


1. 根据您的数据自动进行训练的模型
2. 一个根据Checkpoint(检查点)返回模型体系结构的对象
3. 一种可以自动检测输入语言来加载正确权重的模型

### 5.什么是 TFAutoModel？


1. 根据您的数据自动进行训练的模型
2. 一个根据Checkpoint(检查点)返回模型体系结构的对象
3. 一种可以自动检测输入语言来加载正确权重的模型

### 6.当将不同长度的句子序列在一起批处理时，需要进行哪些处理？


1. 截短
2. 直接将Tensors返回
3. 填充
4. 注意力遮蔽(Attention masking)

### 7.对序列分类(Sequence Classification)模型的 logits 输出应用SoftMax激活函数有什么意义？


1. 它软化了logits输出，使结果更可靠。
2. 它限定了上下界，使模型的输出结果可以被解释。
3. 输出的和是1，从而产生一个可能的概率解释。

### 8.Tokenizer API的核心方法是哪一个？


1. <code>encode</code> ，因为它可以将文本编码为ID，将预测的ID解码为文本
2. 直接调用Tokenizer对象。
3. <code>pad</code>(填充)
4. <code>tokenize</code>(标记)

### 9.这个代码示例中的`result`变量包含什么？

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")
```
1. 字符串列表，每个字符串都是一个标记(Token)
2. 一个ID的列表
3. 包含所有分词后的的字符串

### 10.下面的代码有什么错误吗？

```py
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```
1. 不，看起来是对的。
2. Tokenizer和模型应该来自相同的Checkpoint。
3. 由于模型输入需要是一个Batch，因此可以使用Tokenizer对其进行截断或填充来改进这段代码。

### 10.下面的代码有什么错误吗？

```py
from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = TFAutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```
1. 不，看起来是对的。
2. Tokenizer和模型应该来自相同的Checkpoint。
3. 由于每个输入都是一个Batch，因此可以使用标记器(Tokenizer)对其进行平移和截断来改善这段代码。

---

### 1. 自然语言处理流程的顺序是什么？


正确选项: 3. Tokenizer处理文本并返回 id。模型处理这些 id 并输出一个预测。然后可以再次使用Tokenizer将这些预测转换回某些文本。

1. 首先是模型它处理文本并返回原始预测。然后，分词器会对这些预测进行解释，并在将它们转换回文本   
解析: 模型不能理解文本！必须首先使用Tokenizer将文本转换为id，之后才可以输入给模型。
2. 首先是 Tokenizer，它处理文本并返回id。模型根据这些 id 并输出预测，可以是一些文本。    
解析: 该模型的预测结果是id而不是文本。必须使用Tokenizer才可以将预测转换回文本！
3. Tokenizer处理文本并返回 id。模型处理这些 id 并输出一个预测。然后可以再次使用Tokenizer将这些预测转换回某些文本。    
解析: 正确! Tokenizer可以用于id与文本的相互转换。

### 2. Transformer模型的输出有的张量多少个维度，每个维度分别是什么？


正确选项: 3. 3个维度，分别是: 序列长度(Sequence Length)、批次大小(Batch Size)和隐藏层大小(Hidden Size)

1. 2个维度，分别是: 序列长度(Sequence Length)和批次大小(Batch Size)    
解析: 错! 该模型的张量输出还具有第三个维度: 隐藏层大小(Hidden Size)。
2. 2个维度，分别是: 序列长度(Sequence Length)和隐藏层大小(Hidden Size)    
解析: 错! 所有Transformer模型都批量进行计算，即使是单个序列; 那么也会有批次大小(Batch Size)的维度并且值为1！
3. 3个维度，分别是: 序列长度(Sequence Length)、批次大小(Batch Size)和隐藏层大小(Hidden Size)    
解析: 正确！

### 3.下列哪一个是子词分词的例子（从分词的颗粒度来划分）？


正确选项: 1. WordPiece

正确选项: 4. BPE(Byte Pair Encoding)

正确选项: 5. Unigram

1. WordPiece    
解析: 是的，这是一个子词分词的例子！
2. 基于单个字符的分词    
解析: 基于单个字符的分词和子词分词的颗粒度是不同的。
3. 基于空格和标点符号的分割    
解析: 这是一种基于单词的分词而不是子词分词！
4. BPE(Byte Pair Encoding)    
解析: 是的，这是一个子词分词的的例子！
5. Unigram    
解析: 是的，这是一个子词分词的例子！
6. 以上都不是    
解析: 不对！

### 4.什么是模型头（Haed 层）？


正确选项: 3. 一个附加组件，通常由一个或几个层组成，用于将Transformer的预测转换为特定于任务的输出

1. 原始Transformer网络的一种组件，直接将张量(Tensors)输入到到正确的层    
解析: 不对! 没有这样的组件。
2. 也称为自注意力(self-attention)机制，它会根据序列的其他标记调整一个标记的表示    
解析: 不对！ 自注意力层确实包含“注意力头”，但是和模型头并不是同一个概念。
3. 一个附加组件，通常由一个或几个层组成，用于将Transformer的预测转换为特定于任务的输出    
解析: 没错。它的全名是Adaptation Heads，也被简单地称为模型的头部，在不同的任务上有不同的形式: 语言模型头，问题回答头，序列分类头..。

### 5.什么是AutoModel？


正确选项: 2. 一个根据Checkpoint(检查点)返回模型体系结构的对象

1. 根据您的数据自动进行训练的模型    
解析: 错误。您可能把AutoModel与Hugging Face的<a href='https://huggingface.co/autotrain'>AutoTrain</a> 产品相混淆了？
2. 一个根据Checkpoint(检查点)返回模型体系结构的对象    
解析: 确切地说: <code>AutoModel</code>只需要知道初始化的Checkpoint(检查点)名称就可以返回正确的体系结构。
3. 一种可以自动检测输入语言来加载正确权重的模型    
解析: 不正确; 虽然有些Checkpoint(检查点)和模型能够处理多种语言，但是没有内置的工具可以根据语言自动选择Checkpoint(检查点)。您应该前往 <a href='https://huggingface.co/models'>Model Hub</a> 寻找完成所需任务的最佳Checkpoint(检查点)！

### 5.什么是 TFAutoModel？


正确选项: 2. 一个根据Checkpoint(检查点)返回模型体系结构的对象

1. 根据您的数据自动进行训练的模型    
解析: 错误。您可能把TFAutoModel与Hugging Face的<a href='https://huggingface.co/autotrain'>AutoTrain</a> 产品相混淆了？
2. 一个根据Checkpoint(检查点)返回模型体系结构的对象    
解析: 确切地说: <code>TFAutoModel</code>只需要知道初始化的Checkpoint(检查点)名称就可以返回正确的体系结构。
3. 一种可以自动检测输入语言来加载正确权重的模型    
解析: 不正确; 虽然有些Checkpoint(检查点)和模型能够处理多种语言，但是没有内置的工具可以根据语言自动选择Checkpoint(检查点)。您应该前往 <a href='https://huggingface.co/models'>Model Hub</a> 寻找完成所需任务的最佳Checkpoint(检查点)！

### 6.当将不同长度的句子序列在一起批处理时，需要进行哪些处理？


正确选项: 1. 截短

正确选项: 3. 填充

正确选项: 4. 注意力遮蔽(Attention masking)

1. 截短    
解析: 是的，截断是一个正确的方式，截断可以将他们转化为一个固定长度的矩形序列。这是唯一的正确答案吗？
2. 直接将Tensors返回    
解析: 在批处理序列时不可以直接返回长度不一致的Tensors。(必须是固定长度)
3. 填充    
解析: 是的，填充是一个正确的方式，可以将他们转化为一个固定长度的矩形序列。这是唯一的正确答案吗？
4. 注意力遮蔽(Attention masking)    
解析: 当然！当处理不同长度的序列时，注意力遮蔽是最重要的。然而，仅仅使用注意力遮蔽是不够的。

### 7.对序列分类(Sequence Classification)模型的 logits 输出应用SoftMax激活函数有什么意义？


正确选项: 2. 它限定了上下界，使模型的输出结果可以被解释。

正确选项: 3. 输出的和是1，从而产生一个可能的概率解释。

1. 它软化了logits输出，使结果更可靠。    
解析: 不， SoftMax激活函数不会影响结果的可靠性。
2. 它限定了上下界，使模型的输出结果可以被解释。    
解析: 正确！结果值被压缩到了0和1之间。不过，这并不是我们使用SoftMax激活函数的唯一原因。
3. 输出的和是1，从而产生一个可能的概率解释。    
解析: 没错，但这并不是我们使用SoftMax激活函数的唯一原因。

### 8.Tokenizer API的核心方法是哪一个？


正确选项: 2. 直接调用Tokenizer对象。

1. <code>encode</code> ，因为它可以将文本编码为ID，将预测的ID解码为文本    
解析: 错! 虽然 <code>encode</code> 方法确实是Tokenizer中的方法之一，但是它并不是核心的方法，此外将预测ID解码为文本的是decode。
2. 直接调用Tokenizer对象。    
解析: 完全正确！标记化器(Tokenizer) 的 <code>__call__</code>方法是一个非常强大的方法，可以处理几乎任何事情。它同时也可以从模型中获取预测。
3. <code>pad</code>(填充)    
解析: 错! <code>pad</code>(填充)非常有用，但它只是Tokenizer API的一部分。
4. <code>tokenize</code>(标记)    
解析: 可以说，<code>tokenize</code>(标记)方法是最有用的方法之一，但它不是Tokenizer API的核心方法。

### 9.这个代码示例中的`result`变量包含什么？

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")
```
正确选项: 1. 字符串列表，每个字符串都是一个标记(Token)

1. 字符串列表，每个字符串都是一个标记(Token)    
解析: 正确! 把标记转换成id后，就可以传输给模型了！
2. 一个ID的列表    
解析: 不正确; 这是 <code>__call__</code> 或 <code>convert_tokens_to_ids</code>方法的作用！
3. 包含所有分词后的的字符串    
解析: 这将是次优的答案，因为tokenize方法会将字符串拆分为多个标记的列表。

### 10.下面的代码有什么错误吗？

```py
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```
正确选项: 2. Tokenizer和模型应该来自相同的Checkpoint。

1. 不，看起来是对的。    
解析: 不幸的是，将一个模型与一个不同Checkpoint训练的Tokenizer耦合在并不是一个好主意。模型没有在这个这个Tokenizer上训练来理解Tokenizer的输出，因此模型的输出(如果它可以运行的话)会是错乱的。
2. Tokenizer和模型应该来自相同的Checkpoint。    
解析: 对！
3. 由于模型输入需要是一个Batch，因此可以使用Tokenizer对其进行截断或填充来改进这段代码。    
解析: 的确，模型输入需要是一个Batch。然而，截断或填充这个序列并不一定有意义，这里只有一句话，而截断或填充这些技术是用来批处理一个句子列表使其长度一致的。

### 10.下面的代码有什么错误吗？

```py
from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = TFAutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```
正确选项: 2. Tokenizer和模型应该来自相同的Checkpoint。

1. 不，看起来是对的。    
解析: 不幸的是，将一个模型与一个不同Checkpoint训练的Tokenizer耦合在并不是一个好主意。模型没有在这个这个Tokenizer上训练来理解Tokenizer的输出，因此模型的输出(如果它可以运行的话)会是错乱的。
2. Tokenizer和模型应该来自相同的Checkpoint。    
解析: 对！
3. 由于每个输入都是一个Batch，因此可以使用标记器(Tokenizer)对其进行平移和截断来改善这段代码。 
解析: 的确，每个模型都需要Batch类型的输入。然而，截断或填充这个序列并不一定有意义，这里只有一句话，而这些技术是用来批处理一个句子列表的。

