# 序列-序列模型 [[序列-序列模型]]

<CourseFloatingBanner
    chapter={1}
    classNames="absolute z-10 right-0 top-0"
/>

<Youtube id="0_4KEb08xrE" />

编码器-解码器模型，也称为序列-序列模型，是指同时使用Transformer架构的编码器和解码器两个部分。在每个阶段，编码器的注意力层可以访问输入句子中的所有单词，而解码器的注意力层只能访问将要预测单词前面的单词。

这些模型的预训练可以使用训练编码器或解码器模型的方法来完成，但实际会复杂得多，例如，[T5](https://huggingface.co/t5-base)通过用单个掩码特殊词替换随机文本范围（可能包含多个词）进行预训练，其目标是预测被遮盖单词原始的文本。

序列-序列模型最适合于围绕根据给定输入生成新句子的任务，如摘要、翻译或生成性问答。

该系列模型的典型代表有：

- [BART](https://huggingface.co/transformers/model_doc/bart.html)
- [mBART](https://huggingface.co/transformers/model_doc/mbart.html)
- [Marian](https://huggingface.co/transformers/model_doc/marian.html)
- [T5](https://huggingface.co/transformers/model_doc/t5.html)
