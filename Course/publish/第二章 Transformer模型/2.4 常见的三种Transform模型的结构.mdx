## 2.4 常见的三种Transform模型的结构

常见的三种Transform模型的结构包括编码器模型（Encoder Model）、解码器模型（Decoder Model）和编码器-解码器模型（Encoder-Decoder Model）。

### “编码器”模型（Encoder Model）

编码器模型仅使用 Transformer 模型的编码器部分。在每次计算过程中，注意力层都能访问整个句子的所有单词，这些模型通常具有“双向”（向前/向后）注意力，被称为自编码模型。

这些模型的预训练通常会使用某种方式破坏给定的句子（例如：通过随机遮盖其中的单词），并让模型寻找或重建给定的句子。

“编码器”模型适用于需要理解完整句子的任务，例如：句子分类、命名实体识别（以及更普遍的单词分类）和阅读理解后回答问题。

该系列模型的典型代表有：

- [ALBERT](https://huggingface.co/transformers/model_doc/albert.html)(https://huggingface.co/transformers/model_doc/albert.html) 
- [BERT](https://huggingface.co/transformers/model_doc/bert.html)(https://huggingface.co/transformers/model_doc/bert.html) 
- [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html)(https://huggingface.co/transformers/model_doc/distilbert.html) 
- [ELECTRA](https://huggingface.co/transformers/model_doc/electra.html)(https://huggingface.co/transformers/model_doc/electra.html) 
- [RoBERTa](https://huggingface.co/transformers/model_doc/roberta.html)(https://huggingface.co/transformers/model_doc/roberta.html) 

### “解码器”模型（Decoder Model）

“解码器”模型仅使用 Transformer 模型的解码器部分。在每个阶段，对于给定的单词，注意力层只能获取到句子中位于将要预测单词前面的单词。这些模型通常被称为自回归模型。

“解码器”模型的预训练通常围绕预测句子中的下一个单词进行。

这些模型最适合处理文本生成的任务。

该系列模型的典型代表有：

- [CTRL](https://huggingface.co/transformers/model_doc/ctrl.html)(https://huggingface.co/transformers/model_doc/ctrl.html) 
- [GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)(https://huggingface.co/docs/transformers/model_doc/openai-gpt) 
- [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)(https://huggingface.co/transformers/model_doc/gpt2.html) 
- [Transformer XL](https://huggingface.co/transformers/model_doc/transfor-xl.html)(https://huggingface.co/transformers/model_doc/transfor-xl.html) 

### 编码器-解码器模型（Encoder-Decoder Model）

编码器-解码器模型（也称为序列到序列模型）同时使用 Transformer 架构的编码器和解码器两个部分。在每个阶段，编码器的注意力层可以访问输入句子中的所有单词，而解码器的注意力层只能访问位于输入中将要预测单词前面的单词。

这些模型的预训练可以使用训练编码器或解码器模型的方式来完成，但通常会更加复杂。例如， [T5](https://huggingface.co/t5-base)(https://huggingface.co/t5-base) 通过用单个掩码特殊词替换随机文本范围（可能包含多个词）进行预训练，然后目标是预测被遮盖单词原始的文本。

序列到序列模型最适合于围绕根据给定输入生成新句子的任务，如摘要、翻译或生成性问答。

该系列模型的典型代表有：

- [BART](https://huggingface.co/transformers/model_doc/bart.html)(https://huggingface.co/transformers/model_doc/bart.html) 
- [mBART](https://huggingface.co/transformers/model_doc/mbart.html)(https://huggingface.co/transformers/model_doc/mbart.html) 
- [Marian](https://huggingface.co/transformers/model_doc/marian.html)(https://huggingface.co/transformers/model_doc/marian.html) 
- [T5](https://huggingface.co/transformers/model_doc/t5.html)(https://huggingface.co/transformers/model_doc/t5.html) 
